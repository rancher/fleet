name: CI EKS

on:
  # schedule:
  #   - cron:  '0 4 * * *'
  workflow_dispatch:
    inputs:
      ref:
        description: "checkout git branch/tag"
        required: true
        default: "master"
      keep_cluster:
        description: "Keep the cluster afterwards? (empty/yes)"
        required: false
        default: ""
      aws_id:
        description: "AWS_ACCESS_KEY_ID"
        required: false
        default: ""
      aws_key:
        description: "AWS_SECRET_ACCESS_KEY"
        required: false
        default: ""

env:
  GOARCH: amd64
  CGO_ENABLED: 0
  SETUP_GO_VERSION: '^1.18'
  GINKGO_NODES: 1
  FLAKE_ATTEMPTS: 1
  AWS_REGION: 'us-east-2'
  AWS_MACHINE_TYPE: 't3.xlarge'

jobs:
  eks-fleet-examples:
    runs-on: ubuntu-latest

    steps:
      -
        name: Checkout
        uses: actions/checkout@v3
        with:
          submodules: recursive
          fetch-depth: 0
      -
        name: Setup Go
        uses: actions/setup-go@v3
        with:
          go-version: ${{ env.SETUP_GO_VERSION }}
      -
        name: Setup Ginkgo Test Framework
        run: go install github.com/onsi/ginkgo/v2/ginkgo@v2.1.1
      -
        name: Install Dependencies
        run: |
          brew install kubernetes-cli coreutils
      -
        name: Install EKSCTL
        run: |
          # Better to always use the latest eksctl binary to avoid API version issue
          EKSCTL_GH=https://github.com/weaveworks/eksctl/releases/latest/download
          curl --location ${EKSCTL_GH}/eksctl_$(uname -s)_amd64.tar.gz | tar xz -C .
          chmod +x eksctl
          sudo mv eksctl /usr/local/bin
      -
        name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ github.event.inputs.aws_id || secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ github.event.inputs.aws_key || secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      -
        name: Create EKS cluster
        id: create-cluster
        run: |
          id=$RANDOM
          echo '::set-output name=ID::'$id
          eksctl create cluster --name=fleet-ci$id \
          --region=${{ env.AWS_REGION }} \
          --nodes=2 \
          --node-type=${{ env.AWS_MACHINE_TYPE }} \
          --node-volume-size=40 \
          --managed \
          --kubeconfig=kubeconfig-fleet-ci
          # Workaround for https://github.com/aws/aws-cli/issues/6920
          # https://stackoverflow.com/questions/71318743/kubectl-versions-error-exec-plugin-is-configured-to-use-api-version-client-auth
          sed -i.bak -e 's/v1alpha1/v1beta1/' kubeconfig-fleet-ci
      -
        name: Build fleet binaries
        run: |
          go build -o bin/fleetcontroller-linux-$GOARCH ./cmd/fleetcontroller

          go build -o "bin/fleet-linux-$GOARCH"
          go build -o "bin/fleetagent-linux-$GOARCH" ./cmd/fleetagent
      -
        name: Get UUID
        id: uuid
        run: echo "::set-output name=uuid::$(uuidgen)"
      -
        name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      -
        id: meta-fleet
        uses: docker/metadata-action@v4
        with:
          images: |
            ttl.sh/rancher/fleet-${{ steps.uuid.outputs.uuid }}
          tags: type=raw,value=1h
      -
        uses: docker/build-push-action@v3
        with:
          context: .
          file: package/Dockerfile
          build-args: |
            ARCH=${{ env.GOARCH }}
          push: true
          tags: ${{ steps.meta-fleet.outputs.tags }}
          labels: ${{ steps.meta-fleet.outputs.labels }}
      -
        id: meta-fleet-agent
        uses: docker/metadata-action@v4
        with:
          images: |
            ttl.sh/rancher/fleet-agent-${{ steps.uuid.outputs.uuid }}
          tags: type=raw,value=1h
      -
        uses: docker/build-push-action@v3
        with:
          context: .
          file: package/Dockerfile.agent
          build-args: |
            ARCH=${{ env.GOARCH }}
          push: true
          tags: ${{ steps.meta-fleet-agent.outputs.tags }}
          labels: ${{ steps.meta-fleet-agent.outputs.labels }}
      -
        name: Deploy Fleet
        run: |
          export KUBECONFIG="$GITHUB_WORKSPACE/kubeconfig-fleet-ci"
          echo "${{ steps.meta-fleet.outputs.tags }} ${{ steps.meta-fleet-agent.outputs.tags }}"
          ./.github/scripts/deploy-fleet.sh ${{ steps.meta-fleet.outputs.tags }} ${{ steps.meta-fleet-agent.outputs.tags }}
      -
        name: Fleet Examples Tests
        env:
          FLEET_E2E_NS: fleet-local
        run: |
          export KUBECONFIG="$GITHUB_WORKSPACE/kubeconfig-fleet-ci"
          ginkgo e2e/single-cluster
      -
        name: Fleet Gitrepo Tests
        env:
          FLEET_E2E_NS: fleet-local
          GIT_REPO_URL: "git@github.com:fleetrepoci/testeks.git"
          GIT_REPO_HOST: "github.com"
          GIT_REPO_USER: "git"
        run: |
          export KUBECONFIG="$GITHUB_WORKSPACE/kubeconfig-fleet-ci"
          export GIT_SSH_KEY="$GITHUB_WORKSPACE/id_ecdsa"
          export GIT_SSH_PUBKEY="$GITHUB_WORKSPACE/id_ecdsa.pub"
          echo "${{ secrets.CI_EKS_SSH_KEY }}" > "$GIT_SSH_KEY"
          echo "${{ secrets.CI_EKS_SSH_PUBKEY }}" > "$GIT_SSH_PUBKEY"

          ginkgo e2e/gitrepo
      -
        name: Delete EKS cluster
        # We always tear down the cluster, to avoid costs. Except when running
        # manually and keep_cluster was set to "yes"
        if: ${{ always() && github.event.inputs.keep_cluster != 'yes' }}
        env:
          KUBECONFIG: kubeconfig-fleet-ci
        run: |
          id="${{ steps.create-cluster.outputs.ID }}"
          eksctl delete cluster --region=${{ env.AWS_REGION }} --name=fleet-ci$id
