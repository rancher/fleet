# Upgrade fleet in latest Rancher to dev version and run MC tests
name: E2E Upgrade Fleet in Rancher To HEAD

on:
  schedule:
    # Run everyday day at 1:00 PM
    - cron:  '0 13 * * *'
  workflow_dispatch:
  push:
    tags: [ 'v*' ]
    paths-ignore:
      - '*.md'

env:
  GOARCH: amd64
  CGO_ENABLED: 0
  SETUP_K3D_VERSION: 'v5.8.3'
  SETUP_K3S_VERSION: 'v1.34.1-k3s1'

jobs:
  rancher-fleet-integration:
    runs-on: ubuntu-latest

    steps:
      -
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
        with:
          fetch-depth: 0
      -
        uses: actions/setup-go@4dc6199c7b1a012772edbd06daecab0f50c9053c # v6
        with:
          go-version-file: 'go.mod'
          check-latest: true
      -
        name: Install Ginkgo CLI
        run: go install github.com/onsi/ginkgo/v2/ginkgo
      -
        name: Determine cache key
        id: cache-key
        run: |
          DAY_OF_YEAR=$(date +%j)
          if [ $(($DAY_OF_YEAR % 28)) -eq 0 ]; then
            echo "value=$(date +%Y-%m-%d)" >> $GITHUB_OUTPUT
          else
            echo "value=latest" >> $GITHUB_OUTPUT
          fi
      -
        name: Cache crust-gather CLI
        id: cache-crust
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ~/.local/bin/crust-gather
          key: ${{ runner.os }}-crust-gather-${{ steps.cache-key.outputs.value }}
          restore-keys: |
            ${{ runner.os }}-crust-gather-
      -
        name: Install crust-gather CLI
        run: |
          if [ "${{ steps.cache-crust.outputs.cache-hit }}" != "true" ]; then
            echo "Cache not found, downloading from source"
            mkdir -p ~/.local/bin
            if curl -sSfL https://github.com/crust-gather/crust-gather/raw/main/install.sh | sh -s -- --yes; then
              # Cache the binary for future runs
              if [ ! -f ~/.local/bin/crust-gather ]; then
                which crust-gather && cp $(which crust-gather) ~/.local/bin/
              fi
            else
              echo "Failed to download crust-gather"
              exit 1
            fi
          else
            echo "Using cached crust-gather CLI"
            chmod +x ~/.local/bin/crust-gather
            sudo ln -sf ~/.local/bin/crust-gather /usr/local/bin/
          fi
      -
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        id: rancher-cli-cache
        with:
          path: /home/runner/.local/bin
          key: ${{ runner.os }}-rancher-cli-2.6.0
      -
        name: Install Rancher CLI
        if: steps.rancher-cli-cache.outputs.cache-hit != 'true'
        run: |
          # download an older CLI to avoid https://github.com/rancher/rancher/issues/37574
          mkdir -p /home/runner/.local/bin
          wget -q https://github.com/rancher/cli/releases/download/v2.6.0/rancher-linux-amd64-v2.6.0.tar.gz
          tar -xz --strip-components=2 -f rancher-linux-amd64-v2.6.0.tar.gz -C /home/runner/.local/bin
          rancher --version
      -
        name: Build fleet binaries
        run: |
          ./.github/scripts/build-fleet-binaries.sh
      -
        name: Set up QEMU
        uses: docker/setup-qemu-action@c7c53464625b32c7a7e944ae62b3e17d2b600130 # v3
      -
        name: Set up Docker Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435 # v3
        with:
          driver-opts: |
            image=moby/buildkit:latest
        continue-on-error: true
        id: buildx-setup
      -
        name: Retry Docker Buildx setup if failed
        if: steps.buildx-setup.outcome == 'failure'
        run: |
          echo "Buildx setup failed, waiting 30s and retrying..."
          sleep 30
        continue-on-error: true
      -
        name: Set up Docker Buildx (retry)
        if: steps.buildx-setup.outcome == 'failure'
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435 # v3
        with:
          driver-opts: |
            image=moby/buildkit:latest
      -
        name: Get uuid
        id: uuid
        run: echo "::set-output name=uuid::$(uuidgen)"
      -
        id: meta-fleet
        uses: docker/metadata-action@c299e40c65443455700f0fdfc63efafe5b349051 # v5
        with:
          images: |
            ttl.sh/rancher/fleet-${{ steps.uuid.outputs.uuid }}
          tags: type=raw,value=1h
      -
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83 # v6
        with:
          context: .
          file: package/Dockerfile
          build-args: |
            ARCH=${{ env.GOARCH }}
          push: true
          tags: ${{ steps.meta-fleet.outputs.tags }}
          labels: ${{ steps.meta-fleet.outputs.labels }}
      -
        id: meta-fleet-agent
        uses: docker/metadata-action@c299e40c65443455700f0fdfc63efafe5b349051 # v5
        with:
          images: |
            ttl.sh/rancher/fleet-agent-${{ steps.uuid.outputs.uuid }}
          tags: type=raw,value=1h
      -
        uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83 # v6
        with:
          context: .
          file: package/Dockerfile.agent
          build-args: |
            ARCH=${{ env.GOARCH }}
          push: true
          tags: ${{ steps.meta-fleet-agent.outputs.tags }}
          labels: ${{ steps.meta-fleet-agent.outputs.labels }}
      -
        name: Install k3d
        run: curl --silent --fail https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | TAG=${{ env.SETUP_K3D_VERSION }} bash
      -
        name: Set up k3d control-plane cluster
        run: |
          k3d cluster create upstream --wait \
            -p "80:80@agent:0:direct" \
            -p "443:443@agent:0:direct" \
            --api-port 6443 \
            --agents 1 \
            --k3s-arg '--kubelet-arg=eviction-hard=imagefs.available<1%,nodefs.available<1%@agent:*' \
            --k3s-arg '--kubelet-arg=eviction-minimum-reclaim=imagefs.available=1%,nodefs.available=1%@agent:*' \
            --network "nw01" \
            --image docker.io/rancher/k3s:${{ env.SETUP_K3S_VERSION }}
      -
        name: Set up k3d downstream cluster
        run: |
          k3d cluster create downstream --wait \
            -p "81:80@agent:0:direct" \
            -p "444:443@agent:0:direct" \
            --api-port 6644 \
            --agents 1 \
            --k3s-arg '--kubelet-arg=eviction-hard=imagefs.available<1%,nodefs.available<1%@agent:*' \
            --k3s-arg '--kubelet-arg=eviction-minimum-reclaim=imagefs.available=1%,nodefs.available=1%@agent:*' \
            --network "nw01" \
            --image docker.io/rancher/k3s:${{ env.SETUP_K3S_VERSION }}
      -
        name: Set up Rancher
        run: |
          set -x

          kubectl config use-context k3d-upstream

          # Get the actual IP from traefik LoadBalancer service
          echo "Waiting for traefik LoadBalancer IP..."
          timeout=180
          elapsed=0
          ip=""
          while [ -z "$ip" ] && [ $elapsed -lt $timeout ]; do
            ip=$(kubectl get service -n kube-system traefik -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
            if [ -z "$ip" ]; then
              echo "Still waiting for traefik LoadBalancer IP (${elapsed}s)..."
              sleep 3
              elapsed=$((elapsed + 3))
            fi
          done

          if [ -z "$ip" ]; then
            echo "ERROR: Timeout waiting for traefik LoadBalancer IP after ${elapsed}s"
            echo "Traefik service status:"
            kubectl get service -n kube-system traefik -o yaml
            echo "Kube-system pods:"
            kubectl get pods -n kube-system
            exit 1
          fi

          echo "Traefik LoadBalancer IP: $ip"
          export public_hostname="$ip.sslip.io"

          ./.github/scripts/setup-rancher.sh
          ./.github/scripts/wait-for-loadbalancer.sh
          ./.github/scripts/register-downstream-clusters.sh
          sleep 30
          ./.github/scripts/label-downstream-cluster.sh
      -
        name: Create example workload
        run: |
          kubectl apply -n fleet-local -f e2e/assets/fleet-upgrade/gitrepo-simple.yaml
          kubectl apply -n fleet-default -f e2e/assets/fleet-upgrade/gitrepo-simple.yaml

          echo "Waiting for GitRepos to be processed and bundles created..."

          # Check Fleet system health first
          echo "=== Fleet System Status ==="
          kubectl get deployments -n cattle-fleet-system
          kubectl get pods -n cattle-fleet-system

          # Wait for bundles with timeout and diagnostics
          timeout=600  # 10 minutes
          elapsed=0

          echo "Waiting for fleet-local bundle..."
          while [ $elapsed -lt $timeout ]; do
            if kubectl get bundle -n fleet-local test-simple-simple-chart 2>/dev/null; then
              echo "Bundle found in fleet-local"
              break
            fi
            if [ $((elapsed % 30)) -eq 0 ] && [ $elapsed -gt 0 ]; then
              echo "Still waiting for bundle (${elapsed}s)..."
              echo "Current GitRepos:"
              kubectl get gitrepo -n fleet-local
              echo "Current Bundles:"
              kubectl get bundles -n fleet-local || true
            fi
            sleep 3
            elapsed=$((elapsed + 3))
          done

          if [ $elapsed -ge $timeout ]; then
            echo "ERROR: Timeout waiting for fleet-local bundle"
            echo "GitRepos:"
            kubectl get gitrepo -n fleet-local -o yaml
            echo "Bundles:"
            kubectl get bundles -n fleet-local -o yaml || true
            echo "Fleet controller logs:"
            kubectl logs -n cattle-fleet-system deploy/fleet-controller --tail=100 || true
            exit 1
          fi

          echo "Waiting for fleet-default bundle..."
          elapsed=0
          while [ $elapsed -lt $timeout ]; do
            if kubectl get bundle -n fleet-default test-simple-simple-chart 2>/dev/null; then
              echo "Bundle found in fleet-default"
              break
            fi
            if [ $((elapsed % 30)) -eq 0 ] && [ $elapsed -gt 0 ]; then
              echo "Still waiting for bundle (${elapsed}s)..."
              echo "Current GitRepos:"
              kubectl get gitrepo -n fleet-default
              echo "Current Bundles:"
              kubectl get bundles -n fleet-default || true
            fi
            sleep 3
            elapsed=$((elapsed + 3))
          done

          if [ $elapsed -ge $timeout ]; then
            echo "ERROR: Timeout waiting for fleet-default bundle"
            echo "GitRepos:"
            kubectl get gitrepo -n fleet-default -o yaml
            echo "Bundles:"
            kubectl get bundles -n fleet-default -o yaml || true
            echo "Fleet controller logs:"
            kubectl logs -n cattle-fleet-system deploy/fleet-controller --tail=100 || true
            exit 1
          fi

          echo "Waiting for bundles to be ready..."
          until kubectl get bundles -n fleet-local test-simple-simple-chart -o=jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep -q "True"; do sleep 3; done
          until kubectl get bundles -n fleet-default test-simple-simple-chart -o=jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep -q "True"; do sleep 3; done

          echo "Example workloads created and ready"
      -
        name: Deploy development fleet
        run: |
          echo "${{ steps.meta-fleet.outputs.tags }} ${{ steps.meta-fleet-agent.outputs.tags }}"
          ./.github/scripts/upgrade-rancher-fleet-to-dev-fleet.sh ${{ steps.meta-fleet.outputs.tags }} ${{ steps.meta-fleet-agent.outputs.tags }}

          # Wait for system to stabilize after Fleet upgrade
          echo "Waiting for Fleet system to stabilize after upgrade..."
          kubectl config use-context k3d-upstream

          # Verify fleet-controller is healthy
          kubectl -n cattle-fleet-system wait --for=condition=Available deployment/fleet-controller --timeout=2m

          # IMPORTANT: Wait for fleet-agent on upstream cluster to be ready
          # This is critical for downstream resource cloning feature
          echo "Waiting for fleet-agent on upstream cluster (cattle-fleet-local-system) to be ready..."
          timeout=180
          elapsed=0
          while [ $elapsed -lt $timeout ]; do
            # Check if fleet-agent deployment exists and has ready replicas
            if kubectl get deployment -n cattle-fleet-local-system fleet-agent 2>/dev/null | grep -q "1/1"; then
              echo "fleet-agent deployment is ready"
              break
            fi
            if [ $((elapsed % 30)) -eq 0 ] && [ $elapsed -gt 0 ]; then
              echo "Still waiting for fleet-agent to be ready (${elapsed}s)..."
              kubectl get deployment -n cattle-fleet-local-system fleet-agent 2>/dev/null || echo "Deployment not yet created"
            fi
            sleep 3
            elapsed=$((elapsed + 3))
          done

          if [ $elapsed -ge $timeout ]; then
            echo "WARNING: Timeout waiting for fleet-agent deployment to be ready"
            echo "Checking pod status in cattle-fleet-local-system namespace..."
            kubectl get pods -n cattle-fleet-local-system || true
            kubectl get deployment -n cattle-fleet-local-system fleet-agent -o yaml || true
          fi

          # Also ensure the fleet-agent pod is fully running (not just ready)
          echo "Ensuring fleet-agent pods are fully running..."
          kubectl -n cattle-fleet-local-system wait --for=condition=Ready pod -l app=fleet-agent --timeout=2m 2>/dev/null || true

          # Wait for fleet-agent bundles to be ready on both clusters (upstream local agent)
          echo "Waiting for fleet-agent bundles to reach ready state..."
          timeout=180
          elapsed=0
          while [ $elapsed -lt $timeout ]; do
            local_agent_ready=$(kubectl get bundles -n fleet-local fleet-agent-local -o=jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "False")
            if [ "$local_agent_ready" = "True" ]; then
              echo "fleet-agent-local bundle is ready"
              break
            fi
            if [ $((elapsed % 30)) -eq 0 ] && [ $elapsed -gt 0 ]; then
              echo "Still waiting for fleet-agent-local bundle to be ready (${elapsed}s)..."
              kubectl get bundles -n fleet-local fleet-agent-local || true
            fi
            sleep 3
            elapsed=$((elapsed + 3))
          done

          if [ $elapsed -ge $timeout ]; then
            echo "WARNING: Timeout waiting for fleet-agent-local bundle to be ready (this may affect downstream resource cloning)"
            echo "Current fleet-agent-local bundle status:"
            kubectl get bundles -n fleet-local fleet-agent-local -o yaml || true
            echo "Fleet agent deployment status:"
            kubectl get deployment -n cattle-fleet-local-system fleet-agent -o yaml || true
          fi

          # Verify existing bundles are still ready
          echo "Verifying existing example bundles are still ready..."
          until kubectl get bundles -n fleet-local test-simple-simple-chart -o=jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep -q "True"; do
            echo "Waiting for fleet-local test bundle..."
            sleep 3
          done
          until kubectl get bundles -n fleet-default test-simple-simple-chart -o=jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep -q "True"; do
            echo "Waiting for fleet-default test bundle..."
            sleep 3
          done

          # Verify bundles are being processed by the new controller by checking resource version changes
          echo "Waiting for bundles to be reconciled by new controller..."
          sleep 10

          # Verify fleet-agent on downstream is also updated and healthy
          kubectl config use-context k3d-downstream
          kubectl -n cattle-fleet-system wait --for=condition=Available deployment/fleet-agent --timeout=2m

          # Return to upstream context
          kubectl config use-context k3d-upstream

          # Give extra time for controllers to fully reconcile after upgrade
          echo "Waiting additional 120s for full reconciliation after upgrade..."
          sleep 120

          # Final health check with extended diagnostics
          echo "=== Fleet System Status ==="
          kubectl get pods -n cattle-fleet-system
          echo ""
          echo "=== Bundles Status ==="
          kubectl get bundles -n fleet-local
          kubectl get bundles -n fleet-default
          echo ""
          echo "=== Clusters Status ==="
          kubectl get clusters.fleet.cattle.io -A
          echo ""
          echo "=== BundleDeployments Status ==="
          kubectl get bundledeployments -A | head -20
          echo ""
          echo "=== Fleet Controller Health ==="
          kubectl get deployment -n cattle-fleet-system fleet-controller -o jsonpath='{.status}' | head -c 500
          echo ""
          echo "=== Fleet Agent Status (upstream local) ==="
          kubectl get deployment -n cattle-fleet-local-system fleet-agent -o jsonpath='{.status}' 2>/dev/null | head -c 500 || echo "Not found or not ready"
          echo ""
          echo "Ready to run tests"
      -
        name: E2E tests for examples
        env:
          FLEET_E2E_NS: fleet-local
          FLEET_E2E_NS_DOWNSTREAM: fleet-default
        run: |
          kubectl config use-context k3d-upstream
          export CI_REGISTERED_CLUSTER=$(kubectl get clusters.fleet.cattle.io -n $FLEET_E2E_NS_DOWNSTREAM -o jsonpath='{.items[0].metadata.name}')
          ginkgo --github-output --trace e2e/multi-cluster
      -
        name: Dump Failed Downstream Environment
        if: failure()
        run: |
          kubectl config use-context k3d-downstream
          crust-gather collect --exclude-namespace=kube-system --exclude-kind=Lease --duration=5s -f tmp/downstream

          kubectl config use-context k3d-upstream
          crust-gather collect --exclude-namespace=kube-system --exclude-kind=Lease --duration=5s -f tmp/upstream

          # Additional diagnostics for Fleet
          echo "=== Fleet Controller Logs ==="
          kubectl logs -n cattle-fleet-system deploy/fleet-controller --tail=200 || true

          echo "=== Fleet Bundles ==="
          kubectl get bundles -A || true

          echo "=== Fleet BundleDeployments ==="
          kubectl get bundledeployments -A || true

          echo "=== Fleet Clusters ==="
          kubectl get clusters.fleet.cattle.io -A || true
      -
        name: Upload logs
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
        if: failure()
        with:
          name: gha-fleet-rancher-logs-${{ github.sha }}-${{ github.run_id }}
          path: |
            tmp/downstream
            tmp/upstream
          retention-days: 2
