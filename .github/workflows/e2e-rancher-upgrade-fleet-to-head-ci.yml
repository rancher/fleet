# Test fleet in Rancher with MC tests
name: Test Fleet in Rancher

on:
  schedule:
    # Run everyday at 1:00 PM
    - cron:  '0 13 * * *'
  workflow_call:
    # Variables to set when calling this reusable workflow
    secrets:
      credentials:
        description: Credentials to use to connect
        required: true
      pat_token:
        # A token is needed to be able to add runner on the repo, maybe this can be changed later
        # This token is linked to a personal account
        # So in case of token issue you have to check (no specific order and for example):
        # - the expiration date
        # - if the account associated still exists
        # - if the person still has access to the repo
        description: PAT token used to add runner
        required: true
    inputs:
      ref:
        description: "checkout git branch/tag"
        required: true
        default: "main"
        type: string
      cluster_name:
        description: Name of the provisioned cluster
        required: true
        type: string
      destroy_runner:
        description: Destroy the auto-generated self-hosted runner
        default: true
        type: boolean
      runner_template:
        description: Runner template to use
        default: fleet-qa-e2e-ci-runner-spot-leap-15-5-x86-64-template-v1
        type: string
      zone:
        description: GCP zone to host the runner
        default: europe-west3-a
        type: string
      charts_repo:
        description: Repository from which to source Fleet charts
        default: "fleetrepoci/charts"
        type: string
      charts_branch:
        description: Branch from which to source Fleet charts
        type: string
  push:
    tags: [ 'v*' ]
    paths-ignore:
      - '*.md'

env:
  GOARCH: amd64
  CGO_ENABLED: 0
  SETUP_K3D_VERSION: 'v5.7.4'
  SETUP_K3S_VERSION: 'v1.30.4-k3s1'

jobs:
  create-runner:
    runs-on: ubuntu-latest
    outputs:
      uuid: ${{ steps.generator.outputs.uuid }}
      runner: ${{ steps.generator.outputs.runner }}
      public_dns: ${{ steps.dns.outputs.public_dns }}
    steps:
      # actions/checkout MUST come before auth
      - name: Checkout
        uses: actions/checkout@v3

      - name: Generate UUID and Runner hostname
        id: generator
        run: |
          # Note: keep runner name under 63 characters
          UUID=$(uuidgen)
          echo "uuid=${UUID//-}" >> ${GITHUB_OUTPUT}
          echo "runner=fleet-ci-${UUID//-}" >> ${GITHUB_OUTPUT}

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.credentials }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1

      - name: Create runner
        run: |
          gcloud compute instances create ${{ steps.generator.outputs.runner }} \
            --source-instance-template ${{ inputs.runner_template }} \
            --zone ${{ inputs.zone }}

      - name: Create GCP secrets
        run: |
          echo -n ${{ secrets.pat_token }} \
            | gcloud secrets create PAT_TOKEN_${{ steps.generator.outputs.uuid }} --data-file=-
          echo -n ${{ github.repository }} \
            | gcloud secrets create GH_REPO_${{ steps.generator.outputs.uuid }} --data-file=-

      - name: Get public dns name in GCP
        id: dns
        run: |
          # Do a timed out loop here, as gcloud can sometimes fail
          typeset -i i=0
          while true; do
            # Get public IP
            PUBLIC_IP=$(gcloud compute instances list 2> /dev/null \
                        | awk '/${{ steps.generator.outputs.runner }}/ {print $6}')
            # Exit if we reach the timeout or if IP is set
            if (( ++i > 10 )) || [[ -n "${PUBLIC_IP}" ]]; then
              break
            fi
            # Wait a little before retrying
            sleep 2
          done
          # XXX: do we need this?
          # Get the public DNS
          PUBLIC_DNS=$(host -l ${PUBLIC_IP} 2> /dev/null \
                       | awk '{sub(/\.$/, ""); print $5}')
          echo "public_dns=${PUBLIC_DNS}" >> ${GITHUB_OUTPUT}
          # Raise an error if either IP and/or DNS are empty
          if [[ -z "${PUBLIC_IP}" || -z "${PUBLIC_DNS}" ]]; then
            echo "PUBLIC_IP and/or PUBLIC_DNS are empty!" >&2
            false
          fi

  rancher-integration:
    needs: create-runner
    runs-on: ${{ needs.create-runner.outputs.uuid }}

    steps:
      - name: Add paths into PATH
        run: |
          export PATH=/usr/local/bin:$PATH
          export PATH=/home/gh-runner/.local/bin:$PATH

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.event.inputs.ref }}

      - name: Install Go
        uses: actions/setup-go@v5
        with:
          go-version-file: 'go.mod'
          check-latest: true

      - name: Install Ginkgo CLI
        run: go install github.com/onsi/ginkgo/v2/ginkgo

      - name: Install crust-gather CLI
        run: curl -sSfL https://github.com/crust-gather/crust-gather/raw/main/install.sh | sh -s -- --yes

      - name: Set up build cache
        uses: actions/cache@v4
        id: rancher-cli-cache
        with:
          path: /home/gh-runner/.local/bin
          key: ${{ runner.os }}-rancher-cli

      - name: Install Rancher CLI
        if: steps.rancher-cli-cache.outputs.cache-hit != 'true'
        run: |
          mkdir -p /home/gh-runner/.local/bin
          latest_version=$(curl -ILs -o /dev/null -w %{url_effective} github.com/rancher/cli/releases/latest | sed 's,.*/,,')

          wget -q https://github.com/rancher/cli/releases/download/$latest_version/rancher-linux-amd64-$latest_version.tar.gz
          tar -xz --strip-components=2 -f rancher-linux-amd64-$latest_version.tar.gz -C /usr/local/bin
          rancher --version

      - name: Build fleet binaries
        run: |
          ./.github/scripts/build-fleet-binaries.sh

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Get uuid
        id: uuid
        run: echo "::set-output name=uuid::$(uuidgen)"

      - name: Compute Fleet image metadata
        id: meta-fleet
        uses: docker/metadata-action@v5
        with:
          images: |
            ttl.sh/rancher/fleet-${{ steps.uuid.outputs.uuid }}
          tags: type=raw,value=1h

      - name: Push Fleet image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: package/Dockerfile
          build-args: |
            ARCH=${{ env.GOARCH }}
          push: true
          tags: ${{ steps.meta-fleet.outputs.tags }}
          labels: ${{ steps.meta-fleet.outputs.labels }}

      - name: Compute Fleet agent image metadata
        id: meta-fleet-agent
        uses: docker/metadata-action@v5
        with:
          images: |
            ttl.sh/rancher/fleet-agent-${{ steps.uuid.outputs.uuid }}
          tags: type=raw,value=1h

      - name: Push Fleet agent image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: package/Dockerfile.agent
          build-args: |
            ARCH=${{ env.GOARCH }}
          push: true
          tags: ${{ steps.meta-fleet-agent.outputs.tags }}
          labels: ${{ steps.meta-fleet-agent.outputs.labels }}

      - name: Set up k3d control-plane cluster
        uses: AbsaOSS/k3d-action@v2
        with:
          k3d-version: ${{ env.SETUP_K3D_VERSION }}
          cluster-name: "upstream"
          args: >-
            -p "80:80@agent:0:direct"
            -p "443:443@agent:0:direct"
            --api-port 6443
            --agents 1
            --k3s-arg '--kubelet-arg=eviction-hard=imagefs.available<1%,nodefs.available<1%@agent:*'
            --k3s-arg '--kubelet-arg=eviction-minimum-reclaim=imagefs.available=1%,nodefs.available=1%@agent:*'
            --network "nw01"
            --image docker.io/rancher/k3s:${{ env.SETUP_K3S_VERSION }}

      - name: Set up k3d downstream cluster
        uses: AbsaOSS/k3d-action@v2
        with:
          k3d-version: ${{ env.SETUP_K3D_VERSION }}
          cluster-name: "downstream"
          args: >-
            -p "81:80@agent:0:direct"
            -p "444:443@agent:0:direct"
            --api-port 6644
            --agents 1
            --k3s-arg '--kubelet-arg=eviction-hard=imagefs.available<1%,nodefs.available<1%@agent:*'
            --k3s-arg '--kubelet-arg=eviction-minimum-reclaim=imagefs.available=1%,nodefs.available=1%@agent:*'
            --network "nw01"
            --image docker.io/rancher/k3s:${{ env.SETUP_K3S_VERSION }}
 
            # 1. Clone rancher/rancher
      - name: Checkout rancher/rancher
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          repository: rancher/rancher
          path: rancher

      - name: Set up latest Rancher
        env:
          public_hostname: "172.18.0.1.sslip.io"
        run: |
          kubectl config use-context k3d-upstream

          helm repo add jetstack https://charts.jetstack.io
          helm install cert-manager jetstack/cert-manager \
            --namespace cert-manager \
            --create-namespace \
            --set installCRDs=true \
            --set extraArgs[0]=--enable-certificate-owner-ref=true

          kubectl wait --for=condition=Available deployment --timeout=2m -n cert-manager --all

          helm repo add rancher-latest https://releases.rancher.com/server-charts/latest
          # set CATTLE_SERVER_URL and CATTLE_BOOTSTRAP_PASSWORD to get rancher out of "bootstrap" mode
          helm upgrade rancher rancher-latest/rancher \
            --install --wait \
            --devel \
            --create-namespace \
            --namespace cattle-system \
            --set replicas=1 \
            --set hostname="$public_hostname" \
            --set bootstrapPassword=admin \
            --set "extraEnv[0].name=CATTLE_CHART_DEFAULT_URL" \
            --set "extraEnv[0].value=https://github.com/${{ inputs.charts_repo }}" \
            --set "extraEnv[1].name=CATTLE_CHART_DEFAULT_BRANCH" \
            --set "extraEnv[1].value=${{ inputs.charts_branch }}" \
            --set "extraEnv[2].name=CATTLE_FLEET_VERSION" \
            --set "extraEnv[2].value=999.9.9+up9.9.9"

          # wait for deployment of rancher
          kubectl -n cattle-system rollout status deploy/rancher

          # wait for rancher to create fleet namespace, deployment and controller
          until kubectl get deployments -n cattle-fleet-system | grep -q "fleet"; do sleep 3; done
          kubectl -n cattle-fleet-system rollout status deploy/fleet-controller

          until kubectl get bundles -n fleet-local | grep -q "fleet-agent-local.*1/1"; do sleep 3; done

          helm list -A

      - name: Register Rancher's downstream clusters
        env:
          public_hostname: "172.18.0.1.sslip.io"
        run: |
          ./.github/scripts/wait-for-loadbalancer.sh
          ./.github/scripts/register-downstream-clusters.sh

          # wait for cluster to settle
          sleep 30

          ./.github/scripts/label-downstream-cluster.sh
      -
        name: Create example workload
        run: |
          kubectl apply -n fleet-local -f e2e/assets/fleet-upgrade/gitrepo-simple.yaml
          kubectl apply -n fleet-default -f e2e/assets/fleet-upgrade/gitrepo-simple.yaml
          # wait for bundle ready
          until kubectl get bundles -n fleet-local test-simple-simple-chart -o=jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep -q "True"; do sleep 3; done
          until kubectl get bundles -n fleet-default test-simple-simple-chart -o=jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep -q "True"; do sleep 3; done
      -
        name: Deploy development fleet
        run: |
          echo "${{ steps.meta-fleet.outputs.tags }} ${{ steps.meta-fleet-agent.outputs.tags }}"
          ./.github/scripts/upgrade-rancher-fleet-to-dev-fleet.sh ${{ steps.meta-fleet.outputs.tags }} ${{ steps.meta-fleet-agent.outputs.tags }}

      - name: E2E tests for examples
        env:
          FLEET_E2E_NS: fleet-local
          FLEET_E2E_NS_DOWNSTREAM: fleet-default
        run: |
          kubectl config use-context k3d-upstream
          ginkgo --github-output e2e/multi-cluster

      - name: Dump Failed Downstream Environment
        if: failure()
        run: |
          kubectl config use-context k3d-downstream
          crust-gather collect --exclude-namespace=kube-system --exclude-kind=Lease --duration=5s -f tmp/downstream

      - name: Upload logs
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: gha-fleet-rancher-logs-${{ github.sha }}-${{ github.run_id }}
          path: |
            tmp/downstream
            tmp/upstream
          retention-days: 2

  delete-runner:
    if: ${{ success() }}
    needs: [create-runner, rancher-integration]
    runs-on: ubuntu-latest
    steps:
      # actions/checkout MUST come before auth
      - name: Checkout
        uses: actions/checkout@v3
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.credentials }}
      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1
      - name: Delete GCP secrets
        run: |
          gcloud --quiet secrets delete PAT_TOKEN_${{ needs.create-runner.outputs.uuid }} || true
          gcloud --quiet secrets delete GH_REPO_${{ needs.create-runner.outputs.uuid }} || true
      - name: Delete runner
        if: ${{ always() && needs.create-runner.result == 'success' && inputs.destroy_runner == true }}
        run: |
          gcloud --quiet compute instances delete ${{ needs.create-runner.outputs.runner }} \
            --delete-disks all \
            --zone ${{ inputs.zone }}
